{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------MNIST DATA CLASSIFICATION PYTORCH---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import os \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define  hyperparameters:\n",
    "##### Keep in Mind that there is difference in parameters and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x202f51546d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs =2\n",
    "batch_size=100\n",
    "learning_rate =0.01\n",
    "momentum =0.9\n",
    "log_interval =10\n",
    "\n",
    "random_seed =2046\n",
    "torch.backends.cudnn.enabled =False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download and Load the dataset from torch library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "### Laod MNIST Dataset\n",
    "use_cuda= torch.cuda.is_available()\n",
    "print ( use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normaized datset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Torch Vision offers a lot of transformation, such as cropping or normalization \n",
    "trans_norm= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(1.0,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST(root=root, train =True, transform =trans_norm, download= True)\n",
    "test_set = datasets.MNIST(root=root, train =False, transform =trans_norm, download= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print (len(train_set))\n",
    "print (len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Batch Number: 600\n",
      "Total Test Batch Number: 100\n"
     ]
    }
   ],
   "source": [
    "#Define the batch size\n",
    "batch_size= 100\n",
    "\n",
    "## load the dataset\n",
    "#Data loader. Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size= batch_size, shuffle= True)\n",
    "test_loader= torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print ('Total Training Batch Number: {}'.format(len(train_loader)))\n",
    "print ('Total Test Batch Number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize additional features of DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAABrCAYAAAA/zr2ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACHJJREFUeJzt3W2sTdkdx/Hvn8nFuB5SFa6ph5SbSj2kYlqJSDphIkpkCC+q42ES3rTRvuBNJaZalEbIXI0iEinCVLhM9FXxRkSTCk1lGEW1dVE39dDqzNTDLVZfnLPX3YdePWc5++yzz/19kpP879rn7L3OWfnftfbT2uacQ0RK0yXtCohkkRJHJIASRySAEkckgBJHJIASRySAEkckQGKJY2bXzOztpNafBDOrM7PmfN2dmb2Vdp3SkNG2G5Zvs89jr/eT2t5rSa04w04BTcDBtCsiQfo6554kvZGKDNXM7D0z+62ZfWBm983sL2Y2MV9+w8xum9mi2PtnmNkfzOzT/PIfP7e+hWbWYmb3zOz9+H9IM+tiZj80sz/nlx8wsy8UU0/nXJtzrsk5dwp4Ws7fIKuy0naVVsl9nAnAx0A/4ENgP/B1YAQwH9hiZvX59/4bWAj0BWYA3zWzWQBm9lVgK/Au0AD0Ad6IbecHwCzgm8Ag4J/AL6KFZvaxmX0nma9Ys7LUdi1mdtPMfmlmXwz+xv+Pcy6RF3ANeDsfvwf8KbZsDOCAAbGye8DXOlhXE/BBPv4R8KvYsteBtti2/ghMiS1vAP4DvFZi/W8CbyX1+1TzK4ttB9QDb5Lb/RgANANHk/qNKrmP8/dY/BDAOfd8WT2AmU0AfgaMBuqAbrTvcwwCbkQfcs49MLN7sfUMBT4ys2exsqfkfsy/leWbdD5V33bOuc+Bs1F9zWwp0GpmvZ1znxbzJUtRrYejPwR+DQx2zvUBtgOWX9YKfCl6o5n1IDeEiNwAvuWc6xt7dXfOKWkqo1raLrrs3176rkDVmji9gH845x6Z2TeA+Li2GZiZ30GtA35C4Y+zHfipmQ0FMLP+ZvZOsRs2s25m1j3/Z52ZdTezRH78GpVK25nZBDP7Sv4AQz/g58AJ59y/yvGlnletifM9YLWZfUZuXHwgWuCc+wT4Prkd1FbgM+A28Dj/ls3k/uMdy3/+d+R2bgEws0/M7N2XbPsyuaHHG8DRfDy0PF+rU0ir7b4M/Ca/zgv5dc4r39cqZPkdq8zKH825DzQ65/6adn2keFluu2rtcV7KzGaa2etm1hPYCJwndyRIqlyttF0mEwd4B7iVfzUC33ZZ7zo7j5pou8wP1UTSkNUeRyRVJZ0ANTN1T8W565zrn3Yl4tR2RSuq7dTjJKMl7QpIsKLaTokjEkCJIxJAiSMSQIkjEkCJIxJAiSMSQIkjEkCJIxJAiSMSIJPzqvXo0cPHLS3tJ3qfPcvdqj5w4MCK10k6F/U4IgEy2ePMnTvXx/36tc/1cPv27TSqIyUYNWqUj6dNm+bjFStWAIXt+arOnz8PwNixY8u2zoh6HJEAShyRAJkcqrW2tvq4ra3Nx926dQNg0KBBvuzWrVuVq5h0aPbs2QDs2bPHl3Xp0v5/Ozqw8/jxY1/28OFDH+/evbvkbZ44caLkzxRLPY5IgJLmHKjGuwjjvc+AAQMAWLhwoS/bu3dvxesE/N4592YaG+5I2m13//59AHr37u3L6urqfPzkSe7JHCNGjPBlV69erVDtChTVdupxRAIocUQCZPLgQEfu3LkDQHNzc8o1EWg/IADQq1cvAFatWuXLnj598dldKQ3PSqYeRySAEkckQOaHavEncERHCB89epRWdTq9/v3bpyTbvn27jx88eADAoUOHfFmWZ5FVjyMSIPM9Tpb/a9WS6FaPGTNm+LJ477Njxw4ALl68WNmKJUQ9jkgAJY5IgMwP1eLnbOL36UhlLVmyBIDNmzf7sps3b/p42bJlFa9TktTjiARQ4ogEyPxQrWfPnmlXodOKH0HbtGnTC8vXrVvn4yFDhgBw/fp1Xxad28ki9TgiAWrqfpxIQ0NDCjUpULP348R/26NHj/p49OjRRX0+mkADYN++fT7esGFDGWpXFrofRyQpShyRAJk8OBCfybNr164+jiZ8kPKL5iY7cOCALxs+fLiPjxw5AsDhw4d92enTp308efJkAGbOnOnLli9f7uNx48YBMG/evHJWOzHqcUQCKHFEAmRyqKYpcCsvmi2osbHRl8WHWk1NTS/9/JUrV4DCe3SWLl3q4+hSnfhcaosXL/ZxtV0Frx5HJEAme5xhw4alXYVOZ+XKlUDh3bXHjh17pXVu2bLFx9EVIOvXr/dl165d8/Hq1atfaVvlph5HJIASRyRAJi+5OXfunI/HjBnj4+jggC65eVG1tF1HoklX9u/f78vi53yi8zyXL19Ouiq65EYkKUockQCZPKrWkW3btqVdBQkU7TKsWbPGl02fPt3Ha9euBWDBggW+LM3589TjiATIZI8Tn70z/lSv+FlnSUb01DsoPDBz9uzZsqz/woULPo5P9jFnzhyg8GqF+N2klaYeRySAEkckQCaHavFzT/F7cKrtQsBaNHHiRB9v3LjRx+PHjy/L+uMX7dbX1/s4OkdXLRN8qMcRCZDJHkeqw+DBg8u2rujC3fhVIfEH7UYPRL57927Ztvkq1OOIBFDiiASoqaHaokWLgMKdVklOnz59fDxp0iQATp069dLPTJ061cezZs3y8fz584HCAwLxe3Ne9d6fclOPIxJAiSMSIJNDtSlTpvh4165dPj5z5kwKtelcTp486eOtW7f6eOfOnS8sj18CFR0Viw/F2trafBxN5hENt6FwutxqmzNPPY5IgEzeAZoBne4O0Gimz/iUT3EjR44E4NKlS77s+PHjPj548GCCtSuJ7gAVSYoSRySAhmrJ6HRDtRqioZpIUpQ4IgGUOCIBlDgiAZQ4IgGUOCIBlDgiAUq9yPMu0JJERWrM0LQr8D+o7YpTVNuVdAJURHI0VBMJoMQRCaDEEQmgxBEJoMQRCaDEEQmgxBEJoMQRCaDEEQnwX9gYhDtA0o5MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### DataLoader also contains other features. One of the example is,\n",
    "examples =enumerate(train_loader)\n",
    "batch_idx, (example_data,example_targets)= next(examples)\n",
    "\n",
    "example_data.shape\n",
    "# This gives train data batch is a tensor of shape : 4 [100, 1, 28, 28]. \n",
    "#This means we have 100 examples of 28*28 pixels in grayscale. Lets plot this on matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range (n_epochs):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap= 'gray', interpolation ='none')\n",
    "    plt.title(\"Image: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1= nn.Linear(28*28, 500)\n",
    "        self.fc2=nn.Linear(500,256)\n",
    "        self.fc3=nn.Linear(256,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1, 28*28)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the network and the optimizer:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Init signature:\n",
    "optim.SGD(\n",
    "    ['params', 'lr=<required parameter>', 'momentum=0', 'dampening=0', 'weight_decay=0', 'nesterov=False'],\n",
    ")\n",
    "Docstring:     \n",
    "Implements stochastic gradient descent (optionally with momentum).\n",
    "\n",
    "Nesterov momentum is based on the formula from\n",
    "`On the importance of initialization and momentum in deep learning`__.\n",
    "\n",
    "Args:\n",
    "    params (iterable): iterable of parameters to optimize or dicts defining\n",
    "        parameter groups\n",
    "    lr (float): learning rate\n",
    "    momentum (float, optional): momentum factor (default: 0)\n",
    "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "    dampening (float, optional): dampening for momentum (default: 0)\n",
    "    nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
    "\n",
    "Example:\n",
    "    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    >>> optimizer.zero_grad()\n",
    "    >>> loss_fn(model(input), target).backward()\n",
    "    >>> optimizer.step()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Net()\n",
    "\n",
    "if use_cuda:\n",
    "    model= model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps:\n",
    "  1. Make sure Network is in training mode.\n",
    "  2. Iterate over all training data per epoch. \n",
    "  3. Batch Loading is handled by DataLaoder. \n",
    "  4. Do not Forget to maually set gradient to zero using optimizer.zero_grad(). This is the most common mistake in neural net. \n",
    "  5. Produce the output of Network (Forward Pass). \n",
    "  6. Compute negative lg-likelihood loss (or you can choose you or loss function depending on the nature of problem).\n",
    "  7. Backward Propagation backward() collect the new set of gradients which is propagate back into each parameters using otimizer.step().\n",
    "  8. Plot the training and test lossess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses =[]\n",
    "test_losses = []\n",
    "train_conter =[]\n",
    "test_counter =[i*len(train_loader.dataset) for i in range(n_epochs +1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biswash\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.058788\n",
      "Train Epoch: 0 [1000/60000 (2%)]\tLoss: 0.061251\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 0.108006\n",
      "Train Epoch: 0 [3000/60000 (5%)]\tLoss: 0.091014\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 0.038708\n",
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 0.029328\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 0.095801\n",
      "Train Epoch: 0 [7000/60000 (12%)]\tLoss: 0.046238\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.090315\n",
      "Train Epoch: 0 [9000/60000 (15%)]\tLoss: 0.036589\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.039639\n",
      "Train Epoch: 0 [11000/60000 (18%)]\tLoss: 0.030237\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.025461\n",
      "Train Epoch: 0 [13000/60000 (22%)]\tLoss: 0.024811\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.093226\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 0.092251\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.038741\n",
      "Train Epoch: 0 [17000/60000 (28%)]\tLoss: 0.074864\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.067877\n",
      "Train Epoch: 0 [19000/60000 (32%)]\tLoss: 0.072920\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.064218\n",
      "Train Epoch: 0 [21000/60000 (35%)]\tLoss: 0.038822\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.057486\n",
      "Train Epoch: 0 [23000/60000 (38%)]\tLoss: 0.115677\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.069934\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.072709\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.043695\n",
      "Train Epoch: 0 [27000/60000 (45%)]\tLoss: 0.008228\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.052592\n",
      "Train Epoch: 0 [29000/60000 (48%)]\tLoss: 0.230159\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.041758\n",
      "Train Epoch: 0 [31000/60000 (52%)]\tLoss: 0.068486\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.054555\n",
      "Train Epoch: 0 [33000/60000 (55%)]\tLoss: 0.049634\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.027725\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.092130\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.033552\n",
      "Train Epoch: 0 [37000/60000 (62%)]\tLoss: 0.020612\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.023398\n",
      "Train Epoch: 0 [39000/60000 (65%)]\tLoss: 0.136613\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.038632\n",
      "Train Epoch: 0 [41000/60000 (68%)]\tLoss: 0.097065\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.048667\n",
      "Train Epoch: 0 [43000/60000 (72%)]\tLoss: 0.077782\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.034790\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.095206\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.038941\n",
      "Train Epoch: 0 [47000/60000 (78%)]\tLoss: 0.021304\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.048764\n",
      "Train Epoch: 0 [49000/60000 (82%)]\tLoss: 0.015317\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.023000\n",
      "Train Epoch: 0 [51000/60000 (85%)]\tLoss: 0.045646\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.071424\n",
      "Train Epoch: 0 [53000/60000 (88%)]\tLoss: 0.061340\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.098070\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.063267\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.054225\n",
      "Train Epoch: 0 [57000/60000 (95%)]\tLoss: 0.020240\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.071876\n",
      "Train Epoch: 0 [59000/60000 (98%)]\tLoss: 0.048698\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.071412\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 0.042490\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.032032\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.052419\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.039101\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.061888\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.056703\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 0.054415\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.021307\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.047228\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.133709\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.036006\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.131145\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.027862\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.038190\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.026058\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.070299\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.113788\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.026214\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.048789\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.052282\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.028840\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.037358\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.078304\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.011190\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.082621\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.017354\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.025547\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.071946\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.064616\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.053858\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.055092\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.109892\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.048355\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.016430\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.041596\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.024285\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.047983\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.020584\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.062478\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.038923\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.034857\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.028051\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.031520\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.081269\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.031396\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.121622\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.066153\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.017566\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.008610\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.076446\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.032951\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.099541\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.083961\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.054712\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.046031\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.077236\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.022775\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.055697\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.032271\n"
     ]
    }
   ],
   "source": [
    "## Run the main training loop \n",
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data = data.view(-1, 28*28) # resize data from (100,1,28,28) to (100, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "        loss =criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval ==0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,batch_idx * len(data),len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biswash\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0008, Accuracy: 9758/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0 \n",
    "correct = 0 \n",
    "for data, target in test_loader: \n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data =data.view(-1, 28*28)\n",
    "    net_out = model(data)\n",
    "    \n",
    "    test_loss += criterion(net_out, target).item()\n",
    "    pred = net_out.data.max(1)[1]\n",
    "    correct += pred.eq(target.data).sum()\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
